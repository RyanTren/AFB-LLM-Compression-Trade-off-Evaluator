# ğŸš€ LLM Compression Trade-off Evaluator  

## ğŸ“Œ Project Overview  
This project explores **compression techniques for Large Language Models (LLMs)** and their impact on **inference quality** for **code-generation tasks**.  
The goal is to evaluate trade-offs between model size, accuracy, and efficiency, ultimately enabling **smaller, cost-effective models** without significantly compromising performance.  

## Branches for different compression techniques explored!

- [LoRA/PEFT - Ryan](https://github.com/RyanTren/AFB-LLM-Compression-Trade-off-Evaluator/tree/lora)
- [Quantinization - Aldi](https://github.com/RyanTren/AFB-LLM-Compression-Trade-off-Evaluator/tree/quantization)
- [Knowledge Distilation - Pranav](https://github.com/RyanTren/AFB-LLM-Compression-Trade-off-Evaluator/tree/Knowledge_distillation)

Sponsored by **Robins AFB â€“ AFSC/EN**  
ğŸ”— [Robins AFB Website](https://www.robins.af.mil/)  

---

## ğŸ¯ Objectives  
- Analyze **model compression techniques** (quantization, pruning, distillation, etc.)  
- Evaluate trade-offs in **accuracy vs. speed**  
- Implement **dynamic query routing** between full and compressed models  
- Design test cases for **code generation & debugging**  
- Use **Code-BLEU & BLEU** metrics for evaluation  
- Build a **reporting dashboard** to visualize performance trends  
- Provide **deployment best practices** for efficient cloud-based LLMs  

---

## ğŸ“‚ Scope of Work  
- Set up baseline LLM inference environment  
- Implement multiple **compression strategies**  
- Develop **dynamic query allocation** logic  
- Run experiments on **code-specific datasets**  
- Create an **interactive dashboard** for results  
- Deliver **end-to-end documentation**  

---

## ğŸ“¦ Deliverables  
- ğŸ”§ **Benchmarking scripts** for LLM compression  
- ğŸ”„ **Dynamic query routing** proof-of-concept  
- ğŸ“Š **Code-BLEU metric evaluation reports**  
- ğŸ“ˆ **Interactive visualization dashboard**  
- ğŸ“œ **Final project report** with deployment recommendations  

---

## ğŸ› ï¸ Technology Stack  
- **Python**, **PyTorch**, Hugging Face **Transformers**  
- Compression: **Quantization, Pruning, Distillation**  
- **Docker** for containerized deployment  
- **Streamlit/Dash** for dashboard visualization  
- **GitHub/GitLab** for version control  
- **AWS / GCP / Azure** for cloud testing  

---

## ğŸ“… Project Milestones  
1. **System Setup & Dataset Analysis** (Weeks 1â€“4)  
   - Deliverable: Environment setup + data prep  

2. **Model Compression & Query Routing** (Weeks 5â€“10)  
   - Deliverable: Compression methods + routing logic  

3. **Evaluation, Visualization & Final Report** (Weeks 11â€“15)  
   - Deliverable: Comparative results, dashboard, final report  

---

## ğŸ“ Student Learning Outcomes  
- Hands-on **LLM inference & optimization**  
- Practical knowledge of **compression techniques**  
- End-to-end **full-stack development** (backend + dashboards)  
- Skills in **Python experimentation & evaluation metrics**  
- Experience with **cloud-based AI deployment**  

---

## ğŸ¤ Sponsor Commitment  
Robins AFB (AFSC/EN) provides:  
- Guidance on evaluation metrics  
- Access to example datasets / environments  
- Regular feedback & milestone reviews  

---

## ğŸ“œ License  
This project is developed for academic and research purposes. Licensing details TBD.  
