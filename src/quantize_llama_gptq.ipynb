{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3d369",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quantize a LLaMA-family model to 4-bit GPTQ and save it for later inference.\n",
    "\n",
    "Examples:\n",
    "\n",
    "  # From HF Hub (pass HUGGINGFACE_HUB_TOKEN if gated):\n",
    "  python quantize_llama_gptq.py \\\n",
    "      --model meta-llama/Meta-Llama-3-8B \\\n",
    "      --output /out/llama3-8b-gptq \\\n",
    "      --nsamples 128 --seqlen 512\n",
    "\n",
    "  # From a local model directory on the VM:\n",
    "  python quantize_llama_gptq.py \\\n",
    "      --model /models/llama3.3 \\\n",
    "      --output /out/llama3.3-gptq\n",
    "\"\"\"\n",
    "\n",
    "import argparse, os, json, time, random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "\n",
    "def build_calib_examples(tokenizer, nsamples=128, seqlen=512,\n",
    "                         dataset=\"wikitext\", subset=\"wikitext-2-raw-v1\", split=\"train\"):\n",
    "    \"\"\"\n",
    "    Create a list[dict] of {\"input_ids\",\"attention_mask\"} for AutoGPTQ.quantize.\n",
    "    Keeps RAM small and avoids long tokenization times.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(dataset, subset, split=split)\n",
    "    texts = [t for t in ds[\"text\"] if t and len(t.strip()) > 0]\n",
    "    random.seed(42)\n",
    "    random.shuffle(texts)\n",
    "\n",
    "    examples = []\n",
    "    acc = \"\"\n",
    "    for t in texts:\n",
    "        acc += (\"\\n\\n\" + t)\n",
    "        if len(examples) >= nsamples:\n",
    "            break\n",
    "        toks = tokenizer(acc, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        ids = toks[\"input_ids\"][0]\n",
    "        if ids.numel() >= seqlen:\n",
    "            start = random.randint(0, ids.numel() - seqlen)\n",
    "            chunk = ids[start:start + seqlen].unsqueeze(0)  # [1, seqlen]\n",
    "            examples.append({\n",
    "                \"input_ids\": chunk,\n",
    "                \"attention_mask\": torch.ones_like(chunk)\n",
    "            })\n",
    "            acc = \"\"  # reset accumulator to diversify samples\n",
    "    if len(examples) < nsamples:\n",
    "        print(f\"[WARN] Only built {len(examples)} samples (target {nsamples}).\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(\"LLaMA GPTQ 4-bit quantizer (Maxwell-safe)\")\n",
    "    ap.add_argument(\"--model\", required=True, help=\"HF repo id or local path\")\n",
    "    ap.add_argument(\"--output\", required=True, help=\"Where to save the GPTQ model\")\n",
    "    ap.add_argument(\"--bits\", type=int, default=4, choices=[2,3,4,8],\n",
    "                    help=\"Quantization bits (4 recommended)\")\n",
    "    ap.add_argument(\"--group-size\", type=int, default=128,\n",
    "                    help=\"Grouping for GPTQ (128 is a common sweet spot)\")\n",
    "    ap.add_argument(\"--desc-act\", action=\"store_true\",\n",
    "                    help=\"Quantize with desc_act (slower but can improve quality)\")\n",
    "    ap.add_argument(\"--nsamples\", type=int, default=128,\n",
    "                    help=\"Calibration sample count\")\n",
    "    ap.add_argument(\"--seqlen\", type=int, default=512,\n",
    "                    help=\"Sequence length for calibration\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    os.makedirs(args.output, exist_ok=True)\n",
    "\n",
    "    print(f\"[INFO] Loading tokenizer: {args.model}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)\n",
    "\n",
    "    quant_cfg = BaseQuantizeConfig(\n",
    "        bits=args.bits,\n",
    "        group_size=args.group_size,\n",
    "        desc_act=args.desc_act,\n",
    "    )\n",
    "\n",
    "    print(f\"[INFO] Loading base model into CPU memory (AutoGPTQ recommended default)\")\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        args.model,\n",
    "        quantize_config=quant_cfg,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(f\"[INFO] Building calibration set nsamples={args.nsamples} seqlen={args.seqlen}\")\n",
    "    examples = build_calib_examples(tokenizer, args.nsamples, args.seqlen)\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"[INFO] Starting GPTQ quantization â€¦ this can take a while.\")\n",
    "    # Quantization primarily runs on CPU for safety/compat; it works on Maxwell.\n",
    "    model.quantize(examples)\n",
    "    mins = (time.time() - t0) / 60.0\n",
    "    print(f\"[INFO] Quantization done in {mins:.1f} minutes. Saving to {args.output}\")\n",
    "\n",
    "    model.save_quantized(args.output, use_safetensors=True)\n",
    "    tokenizer.save_pretrained(args.output)\n",
    "\n",
    "    meta = {\n",
    "        \"source_model\": args.model,\n",
    "        \"quantize\": {\"bits\": args.bits, \"group_size\": args.group_size, \"desc_act\": args.desc_act},\n",
    "        \"calibration\": {\"nsamples\": args.nsamples, \"seqlen\": args.seqlen, \"dataset\": \"wikitext-2\"},\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"torch\": torch.__version__,\n",
    "        \"gpu_cc52_note\": \"Built to avoid kernels requiring cc>=7.5; safe for Tesla M40 (Maxwell).\"\n",
    "    }\n",
    "    with open(os.path.join(args.output, \"gptq_metadata.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(\"\\n[TEST LOAD] After this completes you can load with:\")\n",
    "    print(f\"  from auto_gptq import AutoGPTQForCausalLM\")\n",
    "    print(f\"  from transformers import AutoTokenizer\")\n",
    "    print(f\"  model = AutoGPTQForCausalLM.from_quantized('{args.output}', device='cuda:0',\"\n",
    "          f\" use_safetensors=True, use_triton=False)\")\n",
    "    print(f\"  tok = AutoTokenizer.from_pretrained('{args.output}', use_fast=True)\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
