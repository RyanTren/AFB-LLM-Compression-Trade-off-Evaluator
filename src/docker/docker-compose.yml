version: "3.9"
services:
  inference:
    build: ./
    container_name: llama_inference
    shm_size: "16gb"
    volumes:
      - ..:/app
      - ../models:/models
    environment:
      - TRANSFORMERS_CACHE=/models/hf_cache
      - HF_HOME=/models/hf_cache
      - TORCH_CUDA_ARCH_LIST=Maxwell
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
