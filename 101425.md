### Limitations
Tesla M40 GPUs (compute capability ~5.2) are likely to run into compatibility / performance problems with modern bitsandbytes 4-bit / int8 kernels that QLoRA depends on. You can try a few approaches (compile from source, heavy CPU offload, or use smaller models + model-parallel tricks), but the cleanest routes are either (A) use newer GPUs (Ampere/RTX30/40 / A100 family) for QLoRA training or (B) do a hybrid approach (smaller model / inference-only quantization / offline GPTQ) on your M40 cluster. Iâ€™ll give you a concrete checklist + commands, Docker tips, and fallback options so you can try immediately.

- Tesla M40 has compute capability 5.2 (older Maxwell/Kepler generation), while modern bitsandbytes LLM kernels (LLM.int8 / 4-bit NF4 used by QLoRA) target newer CUDA features and higher compute capability GPUs -> this causes unsupported-kernel errors or very poor performance.
- QLoRA (the approach) relies on bitsandbytes 4-bit quantization (NF4) to keep models trainable and tiny; if bitsandbytes GPU kernels cannot load, you lose that path.